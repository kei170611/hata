import numpy as np
from scipy.integrate import solve_ivp
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.impute import KNNImputer
from scipy.ndimage import gaussian_filter1d

# =========================
# 1. Lorenz System Definition ローレンツ系の定義
# =========================
sigma = 10.0
rho   = 28.0
beta  = 8.0 / 3.0

def lorenz(t, xyz):
    x, y, z = xyz
    dx = sigma * (y - x)
    dy = x * (rho - z) - y
    dz = x * y - beta * z
    return [dx, dy, dz]

# =========================
# 2. Time and Initial Conditions 時間設定および初期条件
# =========================
t0, t1 = 0.0, 20.0
dt = 0.01
t_eval = np.arange(t0, t1, dt)
x0 = [1.0, 1.0, 1.0]

# =========================
# 3. True Trajectory 真の軌道
# =========================
sol = solve_ivp(lorenz, (t0, t1), x0, t_eval=t_eval)
t = sol.t
X_true = sol.y.T

# =========================
# 4. Noisy + Missing Data Generation ノイズ付加および欠損データの生成
# =========================
rng = np.random.default_rng(42)
noise_level = 0.02
noise = noise_level * rng.standard_normal(X_true.shape)
X_noisy = X_true + noise

missing_ratio = 0.2
missing_mask = rng.random(X_true.shape) < missing_ratio
X_noisy_masked = X_noisy.copy()
X_noisy_masked[missing_mask] = np.nan

# KNN Imputation
imputer = KNNImputer(n_neighbors=5, weights='distance')
X_imputed = imputer.fit_transform(X_noisy_masked)

print(f"Noise level: {noise_level}, Missing ratio: {missing_ratio}")
print(f"Missing points: {np.sum(missing_mask)} / {X_noisy_masked.size}")

# =========================
# 5. Robust Numerical Differentiation ロバストな数値微分
# =========================
def smooth_derivative(X, dt, sigma_smooth=3):
    """Gaussian smoothing + 2nd order finite difference"""
    X_smooth = np.zeros_like(X)
    for i in range(X.shape[1]):
        X_smooth[:, i] = gaussian_filter1d(X[:, i], sigma=sigma_smooth)
    
    dXdt = np.zeros_like(X_smooth)
    dXdt[1:-1, :] = (X_smooth[2:, :] - X_smooth[:-2, :]) / (2 * dt)
    dXdt[0, :]    = (X_smooth[1, :] - X_smooth[0, :]) / dt
    dXdt[-1, :]   = (X_smooth[-1, :] - X_smooth[-2, :]) / dt
    return dXdt

dXdt = smooth_derivative(X_imputed, dt)

# =========================
# 6. Extended Library Θ(X) 拡張ライブラリ Θ(X)
# =========================
def build_library_extended(X):
    x, y, z = X[:, 0], X[:, 1], X[:, 2]
    ones = np.ones_like(x)
    
    # Normalization for stability
    x_norm = x / (np.std(x) + 1e-8)
    y_norm = y / (np.std(y) + 1e-8)
    z_norm = z / (np.std(z) + 1e-8)
    
    Theta = np.column_stack([
        ones, x_norm, y_norm, z_norm, 
        x_norm*y_norm, x_norm*z_norm, y_norm*z_norm
    ])
    return Theta

Theta = build_library_extended(X_imputed)

# =========================
# 7. Robust SINDy (Ridge + ISTA) ロバスト SINDy（Ridge 初期化＋ISTA）
# =========================
def robust_sindy(Theta, dXdt_col, lam=0.1, max_iter=5000):
    """Noise-robust sparse regression"""
    # Ridge regularization initialization
    alpha_ridge = 0.01
    w0 = np.linalg.inv(Theta.T @ Theta + alpha_ridge * np.eye(Theta.shape[1])) @ Theta.T @ dXdt_col
    
    # Adaptive ISTA step size
    L = np.max(np.linalg.eigvals(Theta.T @ Theta)).real
    eta = 1.0 / L
    
    w = w0.copy()
    for _ in range(max_iter):
        grad = Theta.T @ (Theta @ w - dXdt_col)
        w = np.sign(w - eta * grad) * np.maximum(0, np.abs(w - eta * grad) - lam * eta)
    return w

Xi_robust = np.zeros((Theta.shape[1], 3))
for i in range(3):
    Xi_robust[:, i] = robust_sindy(Theta, dXdt[:, i], lam=0.1)

# =========================
# 8. True vs Estimated Coefficients 真の係数と推定係数の比較
# =========================
terms = ["1", "x", "y", "z", "xy", "xz", "yz"]

# True Lorenz coefficients (pre-normalization reference)
true_Xi = np.array([
    [ 0.0,  0.0,  0.0],      # constant term
    [-10.0, 28.0,  0.0],     # x coefficients
    [10.0, -1.0,  0.0],      # y coefficients
    [ 0.0,  0.0,  0.0],      # z coefficients  
    [ 0.0,  0.0,  1.0],      # xy coefficients
    [ 0.0, -1.0,  0.0],      # xz coefficients
    [ 0.0,  0.0, -8/3]       # yz coefficients
])

df_true = pd.DataFrame(true_Xi, index=terms, columns=['dx/dt', 'dy/dt', 'dz/dt'])
df_est = pd.DataFrame(Xi_robust, index=terms, columns=['dx/dt', 'dy/dt', 'dz/dt'])
df_est_display = df_est.mask(np.abs(df_est) < 0.5, 0)

print("\n=== True Coefficients ===")
print(df_true.round(2))
print("\n=== Estimated Coefficients (Noisy + Missing Data) ===")
print(df_est_display.round(2))
print(f"\nMSE Error: {np.mean((Xi_robust - true_Xi)**2):.4f}")

# =========================
# 9. Model Reconstruction & Validation モデル再構成および検証
# =========================
std_x, std_y, std_z = np.std(X_imputed[:,0]), np.std(X_imputed[:,1]), np.std(X_imputed[:,2])

def lorenz_sindy(t, xyz, Xi):
    x, y, z = xyz
    x_norm = x / (std_x + 1e-8)
    y_norm = y / (std_y + 1e-8)
    z_norm = z / (std_z + 1e-8)
    
    Theta_t = np.array([1.0, x_norm, y_norm, z_norm, 
                       x_norm*y_norm, x_norm*z_norm, y_norm*z_norm])
    return Theta_t @ Xi

sol_sindy = solve_ivp(lambda t, y: lorenz_sindy(t, y, Xi_robust),
                     (t0, t1), x0, t_eval=t_eval, rtol=1e-8)
X_sindy = sol_sindy.y.T

mse_reconstruction = np.mean((X_sindy - X_true)**2, axis=0)
print(f"\nReconstruction MSE: x={mse_reconstruction[0]:.4f}, y={mse_reconstruction[1]:.4f}, z={mse_reconstruction[2]:.4f}")

# =========================
# 10. Visualization (English labels) 可視化（英語ラベル）
# =========================
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Time series comparison
axes[0,0].plot(t, X_true[:,0], 'b-', label='True', alpha=0.8, linewidth=1)
axes[0,0].plot(t, X_imputed[:,0], 'k--', label='Noisy+Imputed', alpha=0.6)
axes[0,0].plot(t, X_sindy[:,0], 'r-', label='SINDy', alpha=0.8)
axes[0,0].set_title('x(t) Time Series')
axes[0,0].set_xlabel('Time t')
axes[0,0].set_ylabel('x')
axes[0,0].legend()

# Phase space
axes[0,1].plot(X_true[:,0], X_true[:,2], 'b-', label='True', alpha=0.8)
axes[0,1].plot(X_sindy[:,0], X_sindy[:,2], 'r-', label='SINDy', alpha=0.8)
axes[0,1].scatter(X_imputed[::50,0], X_imputed[::50,2], c='k', s=1, alpha=0.5, label='Data')
axes[0,1].set_title('x-z Phase Space')
axes[0,1].set_xlabel('x')
axes[0,1].set_ylabel('z')
axes[0,1].legend()

# Coefficient error heatmap
im0 = axes[1,0].imshow(np.abs(Xi_robust - true_Xi), cmap='Reds', aspect='auto')
axes[1,0].set_title('Coefficient Error |Ξ_est - Ξ_true|')
axes[1,0].set_xlabel('Equation (dx/dt, dy/dt, dz/dt)')
axes[1,0].set_ylabel('Library Terms')
plt.colorbar(im0, ax=axes[1,0])

# Missing data visualization
axes[1,1].imshow(missing_mask.T, aspect='auto', cmap='gray', alpha=0.7)
axes[1,1].set_title('Missing Data Locations (20%)')
axes[1,1].set_xlabel('Time Steps')
axes[1,1].set_ylabel('Variables (x,y,z)')

plt.tight_layout()
plt.show()

print("\n===主な改良点のまとめ===")
print("1. ホワイトノイズ（2%）：微分時のノイズ低減のためにガウス平滑化を適用")
print("2. 欠損データ（20%）：時間的相関を考慮した KNN 補完を使用")
print("3. 数値微分：平滑化を併用した 2 次精度の有限差分法")
print("4. ライブラリ：安定性向上のため，多項式次数を削減し正規化を実施")
print("5. SINDy：ロバストなスパース性を得るため，Ridge 初期化＋適応的 ISTA を採用")
